{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "# Summary-like from Tensorflow\n",
    "# from torchinfo import summary\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "# Plot Import\n",
    "import matplotlib.pyplot as plt\n",
    "# For timer\n",
    "import time\n",
    "# Import OS\n",
    "import os\n",
    "from statistics import mean\n",
    "# WFDB\n",
    "import wfdb\n",
    "# ==== LOCAL LIBS ====\n",
    "import ecg_tools_lite as et\n",
    "import evaluator as eva\n",
    "import trainer as tr\n",
    "import models as mod\n",
    "# math\n",
    "import math\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are you trying to do?\n",
    "create_new_model = True\n",
    "create_new_res = True\n",
    "# If create_new_model is True, epochs must have a value\n",
    "epochs = 5\n",
    "# If create_new_model is False, load_model_name must have a value\n",
    "load_model_name = 'saved_model_dict.pt'\n",
    "ecg_saved_res = 'res_pt_full.npy'\n",
    "# Normalization option (either option 0 or 1)\n",
    "norm_option = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New shape: torch.Size([5544, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "ecg_noisy = torch.from_numpy(np.load('all_none_fs1024_e06.npy')).double()\n",
    "ecg_clean = torch.from_numpy(np.load('all_none_fs1024_e24.npy')).double()\n",
    "\n",
    "# Acquire original shape\n",
    "x = ecg_noisy.shape[0]\n",
    "y = ecg_noisy.shape[2]\n",
    "z = ecg_noisy.shape[1]\n",
    "\n",
    "# Reshape\n",
    "ecg_noisy = torch.reshape( ecg_noisy, (x, y, z))\n",
    "ecg_clean = torch.reshape( ecg_clean, (x, y, z))\n",
    "\n",
    "print(f'New shape: {ecg_noisy.shape}')\n",
    "\n",
    "# == Basic Normalization technique (norm from 0 to 1)\n",
    "# ecg_noisy = et.norm_basic( ecg_noisy )\n",
    "# ecg_clean = et.norm_basic( ecg_clean )\n",
    "\n",
    "# == Normalization technique (Norm from -1 to 1)\n",
    "ecg_noisy = et.norm_sig( ecg_noisy )\n",
    "ecg_clean = et.norm_sig( ecg_clean )\n",
    "\n",
    "# ecg_noisy = ecg_noisy.cpu().numpy().flatten()\n",
    "# ecg_clean = ecg_clean.cpu().numpy().flatten()\n",
    "\n",
    "# ecg_noisy = et.realign_all_chunks(ecg_noisy, ecg_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size=kernel_size = 19\n",
    "pad_size = 9\n",
    "class cnn_denoiser(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(cnn_denoiser, self).__init__()\n",
    "\n",
    "        self.denoiser = nn.Sequential(\n",
    "            nn.Conv1d(1, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            nn.Conv1d(36, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            nn.Conv1d(36, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            nn.Conv1d(36, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            nn.Conv1d(36, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            nn.Conv1d(36, 36, kernel_size=kernel_size, stride=1, padding=pad_size),\n",
    "            nn.BatchNorm1d(36),\n",
    "            nn.ReLU(True),\n",
    "            # nn.AvgPool1d(kernel_size=2, stride=4),\n",
    "\n",
    "            # nn.Linear(36, 36)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.denoiser(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 1024])\n",
      "train_size[4435] + valid_size[1109]= 5544\n",
      "same size\n",
      "C:\\python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1, 1024])) that is different to the input size (torch.Size([1, 36, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1 of 2 || time: 302.98 || loss = 0.29342382253745103\n",
      "Epoch 2 of 2 || time: 297.44 || loss = 0.2852555636927421\n",
      "Elapsed time: 600.42, (in mins: 10:0)\n",
      "Validation dataset has not been used: Available validex set size = 1109\n"
     ]
    }
   ],
   "source": [
    "# test_data = torch.tensor([[2,2,2,2]])# np.array([2,2,2])\n",
    "# test_data = torch.from_numpy(test_data)\n",
    "model = cnn_denoiser()\n",
    "model.double()\n",
    "\n",
    "test_data = torch.ones([1, 1, 1024], dtype=torch.double)\n",
    "\n",
    "print( test_data.shape )\n",
    "\n",
    "# train_res = model( test_data )\n",
    "\n",
    "losses = tr.train_model( model=model, epochs=2, ecg_noisy=ecg_noisy, ecg_clean=ecg_clean, train_pct=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Sequential: 1-1                        --\n|    └─Conv1d: 2-1                       720\n|    └─BatchNorm1d: 2-2                  72\n|    └─ReLU: 2-3                         --\n|    └─Conv1d: 2-4                       24,660\n|    └─BatchNorm1d: 2-5                  72\n|    └─ReLU: 2-6                         --\n|    └─Conv1d: 2-7                       24,660\n|    └─BatchNorm1d: 2-8                  72\n|    └─ReLU: 2-9                         --\n|    └─Conv1d: 2-10                      24,660\n|    └─BatchNorm1d: 2-11                 72\n|    └─ReLU: 2-12                        --\n|    └─Conv1d: 2-13                      24,660\n|    └─BatchNorm1d: 2-14                 72\n|    └─ReLU: 2-15                        --\n|    └─Conv1d: 2-16                      24,660\n|    └─BatchNorm1d: 2-17                 72\n|    └─ReLU: 2-18                        --\n=================================================================\nTotal params: 124,452\nTrainable params: 124,452\nNon-trainable params: 0\n=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv1d: 2-1                       720\n",
       "|    └─BatchNorm1d: 2-2                  72\n",
       "|    └─ReLU: 2-3                         --\n",
       "|    └─Conv1d: 2-4                       24,660\n",
       "|    └─BatchNorm1d: 2-5                  72\n",
       "|    └─ReLU: 2-6                         --\n",
       "|    └─Conv1d: 2-7                       24,660\n",
       "|    └─BatchNorm1d: 2-8                  72\n",
       "|    └─ReLU: 2-9                         --\n",
       "|    └─Conv1d: 2-10                      24,660\n",
       "|    └─BatchNorm1d: 2-11                 72\n",
       "|    └─ReLU: 2-12                        --\n",
       "|    └─Conv1d: 2-13                      24,660\n",
       "|    └─BatchNorm1d: 2-14                 72\n",
       "|    └─ReLU: 2-15                        --\n",
       "|    └─Conv1d: 2-16                      24,660\n",
       "|    └─BatchNorm1d: 2-17                 72\n",
       "|    └─ReLU: 2-18                        --\n",
       "=================================================================\n",
       "Total params: 124,452\n",
       "Trainable params: 124,452\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "summary( cnn_denoiser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 31064850432 bytes. Buy new RAM!",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b1cd48747ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mecg_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdenoiser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mecg_noisy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34mf'ecg_res shape: {ecg_res.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mec_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mecg_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    256\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 258\u001b[1;33m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    259\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 31064850432 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "ecg_res = model.denoiser(ecg_noisy)\n",
    "print( f'ecg_res shape: {ecg_res.shape}')\n",
    "ec_res = ecg_res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.ecg_plot([ecg_clean, ecg_res], ['ECG Clean', 'ECG Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pct = 0.8\n",
    "# def get_train_valid_sizes( ecg_clean ):\n",
    "#     # valid_pct = 1 - train_pct # auto compute for the validation\n",
    "#     # Compute for the sizes of training and validation\n",
    "#     total_size = ecg_clean.shape[0]\n",
    "#     train_size = int( (total_size) * train_pct )\n",
    "#     valid_size = total_size - train_size\n",
    "    \n",
    "#     print( f'train_size[{train_size}] + valid_size[{valid_size}]= {total_size}')\n",
    "#     if( (train_size+valid_size) == total_size):\n",
    "#         print(f'same size')\n",
    "#     else:\n",
    "#         print(f'not same size')\n",
    "#     return train_size, valid_size, total_size\n",
    "\n",
    "# # CREATE THE TRAINDEX (TRAINING INDEX SETS)\n",
    "# def create_index_loaders( ecg_clean ):\n",
    "#     # Get train and validation set sizes\n",
    "#     train_size, valid_size, total_size = get_train_valid_sizes(ecg_clean)    \n",
    "#     # Well instead of having to randomize the data itself, why not the numbers used to index\n",
    "#     index_set = np.arange(0, total_size)\n",
    "\n",
    "#     # split the indexes used for training and validation\n",
    "#     train_indexset, val_indexset = random_split( index_set, (train_size, valid_size))\n",
    "\n",
    "#     # Create DataLoaders for the corresponding train and val sets\n",
    "#     traindex_loader = DataLoader( train_indexset, shuffle=True, batch_size=1 )\n",
    "#     validex_loader = DataLoader( val_indexset, shuffle=True, batch_size=1 )\n",
    "#     return traindex_loader, validex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindex_loader, validex_loader = create_index_loaders( ecg_clean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cnn_denoiser().cuda()\n",
    "# model.double()\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for i, data in enumerate(traindex_loader):\n",
    "#         index = data.numpy()[0]\n",
    "#         clean_samp = ecg_clean[index]\n",
    "\n",
    "#         clean_samp = clean_samp.view(1, clean_samp.shape[0], clean_samp.shape[1])\n",
    "#         print(clean_samp.shape)\n",
    "#         train_res = model( clean_samp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}