{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ed5bc5017c3fb04814a932759475882729cab95ed5b05095733f3a83ad7fc497"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PyTorch Test for ECG Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "# Plot Import\n",
    "import matplotlib.pyplot as plt\n",
    "# For timer\n",
    "import time\n",
    "# Import OS\n",
    "import os\n",
    "# import local libs\n",
    "import ecg_tools_lite as et\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " I haz cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\" I haz cuda\")\n",
    "else:\n",
    "    print(\"Haz no cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "# dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(360, 360, 16),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(360, 360, 16),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "                model.parameters()\n",
    "            ) # lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_clean = et.load_ecg_file('all_all_fs360_e24.npy')\n",
    "ecg_noisy = et.load_ecg_file('all_all_fs360_e06.npy')\n",
    "\n",
    "trainloader = DataLoader( ecg_noisy, shuffle=True, num_workers=2 )\n",
    "# testloader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.0051],\n",
       "         [ 0.0031],\n",
       "         [-0.0012],\n",
       "         [-0.0021],\n",
       "         [-0.0041],\n",
       "         [-0.0089],\n",
       "         [-0.0065],\n",
       "         [-0.0070],\n",
       "         [-0.0079],\n",
       "         [-0.0074],\n",
       "         [-0.0108],\n",
       "         [-0.0132],\n",
       "         [-0.0137],\n",
       "         [-0.0127],\n",
       "         [-0.0151],\n",
       "         [-0.0166],\n",
       "         [-0.0204],\n",
       "         [-0.0214],\n",
       "         [-0.0233],\n",
       "         [-0.0243],\n",
       "         [-0.0243],\n",
       "         [-0.0243],\n",
       "         [-0.0248],\n",
       "         [-0.0262],\n",
       "         [-0.0277],\n",
       "         [-0.0310],\n",
       "         [-0.0334],\n",
       "         [-0.0392],\n",
       "         [-0.0445],\n",
       "         [-0.0479],\n",
       "         [-0.0498],\n",
       "         [-0.0512],\n",
       "         [-0.0527],\n",
       "         [-0.0527],\n",
       "         [-0.0512],\n",
       "         [-0.0512],\n",
       "         [-0.0527],\n",
       "         [-0.0551],\n",
       "         [-0.0560],\n",
       "         [-0.0541],\n",
       "         [-0.0546],\n",
       "         [-0.0512],\n",
       "         [-0.0503],\n",
       "         [-0.0493],\n",
       "         [-0.0483],\n",
       "         [-0.0435],\n",
       "         [-0.0431],\n",
       "         [-0.0406],\n",
       "         [-0.0397],\n",
       "         [-0.0382],\n",
       "         [-0.0387],\n",
       "         [-0.0397],\n",
       "         [-0.0378],\n",
       "         [-0.0344],\n",
       "         [-0.0349],\n",
       "         [-0.0325],\n",
       "         [-0.0315],\n",
       "         [-0.0310],\n",
       "         [-0.0301],\n",
       "         [-0.0286],\n",
       "         [-0.0281],\n",
       "         [-0.0267],\n",
       "         [-0.0281],\n",
       "         [-0.0301],\n",
       "         [-0.0310],\n",
       "         [-0.0296],\n",
       "         [-0.0277],\n",
       "         [-0.0219],\n",
       "         [-0.0166],\n",
       "         [-0.0151],\n",
       "         [-0.0185],\n",
       "         [-0.0156],\n",
       "         [-0.0252],\n",
       "         [-0.0281],\n",
       "         [-0.0277],\n",
       "         [-0.0257],\n",
       "         [-0.0248],\n",
       "         [-0.0233],\n",
       "         [-0.0228],\n",
       "         [-0.0180],\n",
       "         [-0.0113],\n",
       "         [-0.0041],\n",
       "         [-0.0017],\n",
       "         [ 0.0007],\n",
       "         [ 0.0027],\n",
       "         [ 0.0022],\n",
       "         [-0.0017],\n",
       "         [-0.0070],\n",
       "         [-0.0151],\n",
       "         [-0.0200],\n",
       "         [-0.0214],\n",
       "         [-0.0252],\n",
       "         [-0.0272],\n",
       "         [-0.0277],\n",
       "         [-0.0296],\n",
       "         [-0.0301],\n",
       "         [-0.0310],\n",
       "         [-0.0286],\n",
       "         [-0.0248],\n",
       "         [-0.0204],\n",
       "         [-0.0147],\n",
       "         [-0.0098],\n",
       "         [-0.0036],\n",
       "         [ 0.0012],\n",
       "         [ 0.0080],\n",
       "         [ 0.0104],\n",
       "         [ 0.0108],\n",
       "         [ 0.0104],\n",
       "         [ 0.0094],\n",
       "         [ 0.0099],\n",
       "         [ 0.0075],\n",
       "         [ 0.0060],\n",
       "         [ 0.0060],\n",
       "         [ 0.0060],\n",
       "         [ 0.0056],\n",
       "         [ 0.0065],\n",
       "         [ 0.0084],\n",
       "         [ 0.0137],\n",
       "         [ 0.0166],\n",
       "         [ 0.0210],\n",
       "         [ 0.0262],\n",
       "         [ 0.0267],\n",
       "         [ 0.0282],\n",
       "         [ 0.0253],\n",
       "         [ 0.0272],\n",
       "         [ 0.0248],\n",
       "         [ 0.0219],\n",
       "         [ 0.0219],\n",
       "         [ 0.0185],\n",
       "         [ 0.0171],\n",
       "         [ 0.0157],\n",
       "         [ 0.0210],\n",
       "         [ 0.0243],\n",
       "         [ 0.0267],\n",
       "         [ 0.0272],\n",
       "         [ 0.0272],\n",
       "         [ 0.0248],\n",
       "         [ 0.0253],\n",
       "         [ 0.0287],\n",
       "         [ 0.0330],\n",
       "         [ 0.0392],\n",
       "         [ 0.0537],\n",
       "         [ 0.0768],\n",
       "         [ 0.0609],\n",
       "         [ 0.1432],\n",
       "         [ 0.1629],\n",
       "         [ 0.1716],\n",
       "         [ 0.1774],\n",
       "         [ 0.1826],\n",
       "         [ 0.1899],\n",
       "         [ 0.1937],\n",
       "         [ 0.1961],\n",
       "         [ 0.1961],\n",
       "         [ 0.1928],\n",
       "         [ 0.1889],\n",
       "         [ 0.1826],\n",
       "         [ 0.1740],\n",
       "         [ 0.1663],\n",
       "         [ 0.1595],\n",
       "         [ 0.1543],\n",
       "         [ 0.1523],\n",
       "         [ 0.1480],\n",
       "         [ 0.1451],\n",
       "         [ 0.1374],\n",
       "         [ 0.1326],\n",
       "         [ 0.1273],\n",
       "         [ 0.1201],\n",
       "         [ 0.1124],\n",
       "         [ 0.1052],\n",
       "         [ 0.0965],\n",
       "         [ 0.0878],\n",
       "         [ 0.0758],\n",
       "         [ 0.0638],\n",
       "         [ 0.0503],\n",
       "         [ 0.0388],\n",
       "         [ 0.0282],\n",
       "         [ 0.0195],\n",
       "         [ 0.0108],\n",
       "         [-0.0021],\n",
       "         [ 0.0022],\n",
       "         [-0.0175],\n",
       "         [-0.0243],\n",
       "         [-0.0315],\n",
       "         [-0.0387],\n",
       "         [-0.0421],\n",
       "         [-0.0455],\n",
       "         [-0.0503],\n",
       "         [-0.0527],\n",
       "         [-0.0517],\n",
       "         [-0.0421],\n",
       "         [-0.0431],\n",
       "         [-0.0426],\n",
       "         [-0.0455],\n",
       "         [-0.0469],\n",
       "         [-0.0483],\n",
       "         [-0.0522],\n",
       "         [-0.0498],\n",
       "         [-0.0512],\n",
       "         [-0.0522],\n",
       "         [-0.0508],\n",
       "         [-0.0498],\n",
       "         [-0.0527],\n",
       "         [-0.0609],\n",
       "         [-0.0748],\n",
       "         [-0.0787],\n",
       "         [-0.0743],\n",
       "         [-0.0599],\n",
       "         [-0.0609],\n",
       "         [-0.0710],\n",
       "         [-0.0772],\n",
       "         [-0.0666],\n",
       "         [-0.0589],\n",
       "         [-0.0686],\n",
       "         [-0.0676],\n",
       "         [-0.0666],\n",
       "         [-0.0676],\n",
       "         [-0.0666],\n",
       "         [-0.0637],\n",
       "         [-0.0546],\n",
       "         [-0.0498],\n",
       "         [-0.0512],\n",
       "         [-0.0560],\n",
       "         [-0.0556],\n",
       "         [-0.0565],\n",
       "         [-0.0609],\n",
       "         [-0.0686],\n",
       "         [-0.0753],\n",
       "         [-0.0816],\n",
       "         [-0.0873],\n",
       "         [-0.0931],\n",
       "         [-0.1022],\n",
       "         [-0.1124],\n",
       "         [-0.1162],\n",
       "         [-0.1172],\n",
       "         [-0.1152],\n",
       "         [-0.1104],\n",
       "         [-0.1066],\n",
       "         [-0.1008],\n",
       "         [-0.1008],\n",
       "         [-0.1061],\n",
       "         [-0.1109],\n",
       "         [-0.1124],\n",
       "         [-0.1138],\n",
       "         [-0.1157],\n",
       "         [-0.1181],\n",
       "         [-0.1104],\n",
       "         [-0.0941],\n",
       "         [-0.0791],\n",
       "         [-0.0787],\n",
       "         [-0.0820],\n",
       "         [-0.0840],\n",
       "         [-0.0849],\n",
       "         [-0.0864],\n",
       "         [-0.0864],\n",
       "         [-0.0888],\n",
       "         [-0.0849],\n",
       "         [-0.0849],\n",
       "         [-0.0854],\n",
       "         [-0.0806],\n",
       "         [-0.0787],\n",
       "         [-0.0743],\n",
       "         [-0.0710],\n",
       "         [-0.0613],\n",
       "         [-0.0488],\n",
       "         [-0.0349],\n",
       "         [-0.0301],\n",
       "         [-0.0267],\n",
       "         [-0.0267],\n",
       "         [-0.0277],\n",
       "         [-0.0286],\n",
       "         [-0.0291],\n",
       "         [-0.0281],\n",
       "         [-0.0291],\n",
       "         [-0.0301],\n",
       "         [-0.0315],\n",
       "         [-0.0344],\n",
       "         [-0.0421],\n",
       "         [-0.0527],\n",
       "         [-0.0618],\n",
       "         [-0.0662],\n",
       "         [-0.0686],\n",
       "         [-0.0652],\n",
       "         [-0.0637],\n",
       "         [-0.0647],\n",
       "         [-0.0666],\n",
       "         [-0.0666],\n",
       "         [-0.0700],\n",
       "         [-0.0662],\n",
       "         [-0.0772],\n",
       "         [-0.0782],\n",
       "         [-0.0758],\n",
       "         [-0.0791],\n",
       "         [-0.0806],\n",
       "         [-0.0811],\n",
       "         [-0.0830],\n",
       "         [-0.0844],\n",
       "         [-0.0868],\n",
       "         [-0.0883],\n",
       "         [-0.0902],\n",
       "         [-0.0917],\n",
       "         [-0.0941],\n",
       "         [-0.0970],\n",
       "         [-0.1022],\n",
       "         [-0.1095],\n",
       "         [-0.1133],\n",
       "         [-0.1157],\n",
       "         [-0.1176],\n",
       "         [-0.1162],\n",
       "         [-0.1172],\n",
       "         [-0.1191],\n",
       "         [-0.1172],\n",
       "         [-0.1157],\n",
       "         [-0.1138],\n",
       "         [-0.1119],\n",
       "         [-0.1080],\n",
       "         [-0.1071],\n",
       "         [-0.1095],\n",
       "         [-0.1085],\n",
       "         [-0.1090],\n",
       "         [-0.1066],\n",
       "         [-0.1051],\n",
       "         [-0.1042],\n",
       "         [-0.1051],\n",
       "         [-0.1066],\n",
       "         [-0.1047],\n",
       "         [-0.1051],\n",
       "         [-0.1061],\n",
       "         [-0.1143],\n",
       "         [-0.1196],\n",
       "         [-0.1225],\n",
       "         [-0.1234],\n",
       "         [-0.1249],\n",
       "         [-0.1249],\n",
       "         [-0.1229],\n",
       "         [-0.1278],\n",
       "         [-0.1335],\n",
       "         [-0.1374],\n",
       "         [-0.1383],\n",
       "         [-0.1383],\n",
       "         [-0.1388],\n",
       "         [-0.1374],\n",
       "         [-0.1393],\n",
       "         [-0.1417],\n",
       "         [-0.1431],\n",
       "         [-0.1470],\n",
       "         [-0.1523],\n",
       "         [-0.1552],\n",
       "         [-0.1552],\n",
       "         [-0.1552],\n",
       "         [-0.1566],\n",
       "         [-0.1614],\n",
       "         [-0.1634],\n",
       "         [-0.1653],\n",
       "         [-0.1658],\n",
       "         [-0.1648],\n",
       "         [-0.1634],\n",
       "         [-0.1653],\n",
       "         [-0.1648],\n",
       "         [-0.1629],\n",
       "         [-0.1624]]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added elapsed timer\n",
    "elapsed_start = time.time() #timer\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time() # timer\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        noisy_img = add_noise(img)\n",
    "        noisy_img = Variable(noisy_img).cuda()\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(noisy_img)\n",
    "        loss = criterion(output, img)\n",
    "        MSE_loss = nn.MSELoss()(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data.item(), MSE_loss.data.item()\n",
    "          )\n",
    "          )\n",
    "    epoch_end = time.time()\n",
    "    print(\"Epoch run time: %.2f\" %(epoch_end-epoch_start))\n",
    "    if epoch % 10 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        x_noisy = to_img(noisy_img.cpu().data)\n",
    "        weights = to_img(model.encoder[0].weight.cpu().data)\n",
    "        save_image(x, './mlp_img/x_{}.png'.format(epoch))\n",
    "        save_image(x_hat, './mlp_img/x_hat_{}.png'.format(epoch))\n",
    "        save_image(x_noisy, './mlp_img/x_noisy_{}.png'.format(epoch))\n",
    "        save_image(weights, './filters/epoch_{}.png'.format(epoch))\n",
    "\n",
    "elapsed_end = time.time()\n",
    "elapsed_total = elapsed_end-elapsed_start\n",
    "print(\"Total elapsed time: %.2f, (in minutes %3d)\" %(elapsed_total, (elapsed_total/60)))\n",
    "\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  }
 ]
}